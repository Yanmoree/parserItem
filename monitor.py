# monitor.py - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –º–µ—Ç–æ–¥–∞–º–∏ run() –∏ stop()
import requests
from bs4 import BeautifulSoup
import time
import re
import asyncio
from typing import List, Dict
from datetime import datetime, timedelta

class GoofishMonitor:
    def __init__(self, bot=None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∞ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Å—Å—ã–ª–∫–æ–π –Ω–∞ –±–æ—Ç–∞"""
        self.bot = bot
        self.is_running = False
        self.base_url = "https://goofish.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        })
        print("‚úÖ GoofishMonitor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def run(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        self.is_running = True
        print("üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Goofish –∑–∞–ø—É—â–µ–Ω")
        
        while self.is_running:
            try:
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
                print("üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∞–∫—Ç–∏–≤–µ–Ω...")
                await asyncio.sleep(60)  # –ü–∞—É–∑–∞ 60 —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
                await asyncio.sleep(30)  # –ñ–¥–µ–º 30 —Å–µ–∫—É–Ω–¥ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        self.is_running = False
        print("üõë –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def get_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        return {
            'is_running': self.is_running,
            'status': 'active' if self.is_running else 'stopped',
            'monitor_type': 'GoofishMonitor'
        }
    
    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –≤–∞—à–∏ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π:
    def parse_product_details(self, product_url: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–µ"""
        try:
            response = self.session.get(product_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞
            images = []
            img_elements = soup.find_all('img', {'src': re.compile(r'\.(jpg|jpeg|png|webp)')})
            
            for img in img_elements[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 —Ñ–æ—Ç–æ
                img_url = img.get('src')
                if img_url and img_url.startswith('http'):
                    if img_url not in images:
                        images.append(img_url)
            
            # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª
            description = ""
            desc_elements = soup.find_all(['div', 'p'], class_=re.compile(r'desc|description|detail'))
            for elem in desc_elements:
                if elem.text:
                    description += elem.text + " "
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            original_keywords = ['–æ—Ä–∏–≥–∏–Ω–∞–ª', 'original', 'genuine', '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π', '–∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π', 
                                '–∑–∞–≤–æ–¥—Å–∫–æ–π', 'brand new', 'new', '–Ω–æ–≤—ã–π']
            is_original = any(keyword.lower() in description.lower() for keyword in original_keywords)
            
            return {
                'images': images[:10],  # –ú–∞–∫—Å–∏–º—É–º 10 —Ñ–æ—Ç–æ
                'description': description[:500],  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
                'is_original': is_original,
                'detailed_url': product_url
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–µ—Ç–∞–ª–µ–π —Ç–æ–≤–∞—Ä–∞: {e}")
            return {
                'images': [],
                'description': '',
                'is_original': False,
                'detailed_url': product_url
            }
    
    def search_all_pages(self, query: str, max_minutes: int = 60) -> List[Dict]:
        """–ò—â–µ—Ç —Ç–æ–≤–∞—Ä—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        products = []
        
        try:
            # –≠–Ω–∫–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å –¥–ª—è URL
            encoded_query = requests.utils.quote(query)
            url = f"{self.base_url}/search?q={encoded_query}&sort=new"
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤
            product_cards = soup.find_all('div', class_=re.compile(r'product|item|card'))
            
            for card in product_cards[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 20 —Ç–æ–≤–∞—Ä–∞–º–∏
                try:
                    product = self._parse_product_card(card, query)
                    if product:
                        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                        details = self.parse_product_details(product['url'])
                        product.update(details)
                        
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                        if product['age_minutes'] <= max_minutes:
                            products.append(product)
                            
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
                    continue
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–≤–∏–∑–Ω–µ
            products.sort(key=lambda x: x['age_minutes'])
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤: {e}")
        
        return products
    
    def _parse_product_card(self, card, search_query: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç –∫–∞—Ä—Ç–æ—á–∫—É —Ç–æ–≤–∞—Ä–∞"""
        try:
            # –ù–∞–∑–≤–∞–Ω–∏–µ –∏ —Å—Å—ã–ª–∫–∞
            title_elem = card.find('a', href=True)
            if not title_elem:
                return None
                
            title = title_elem.text.strip()
            url = title_elem['href']
            if not url.startswith('http'):
                url = self.base_url + url
            
            # –¶–µ–Ω–∞
            price_elem = card.find(class_=re.compile(r'price|cost|amount'))
            price_text = price_elem.text.strip() if price_elem else "0"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Ü–µ–Ω—ã
            price_match = re.search(r'[\d,\.]+', price_text.replace(',', ''))
            price_yuan = float(price_match.group()) if price_match else 0
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ä—É–±–ª–∏ (–ø—Ä–∏–º–µ—Ä–Ω—ã–π –∫—É—Ä—Å ~12.5)
            price_rub = round(price_yuan * 12.5, 2)
            
            # ID —Ç–æ–≤–∞—Ä–∞ (–∏–∑ URL –∏–ª–∏ —Ö—ç—à)
            product_id = str(abs(hash(url)))  # –ü—Ä–æ—Å—Ç–æ–π —Ö—ç—à
            
            # –õ–æ–∫–∞—Ü–∏—è
            location_elem = card.find(class_=re.compile(r'location|city|place'))
            location = location_elem.text.strip() if location_elem else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            
            # –í–æ–∑—Ä–∞—Å—Ç —Ç–æ–≤–∞—Ä–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
            age_elem = card.find(class_=re.compile(r'time|age|date'))
            age_text = age_elem.text.strip() if age_elem else "1 —á–∞—Å"
            
            # –ü–∞—Ä—Å–∏–º –≤–æ–∑—Ä–∞—Å—Ç (–ø—Ä–∏–º–µ—Ä: "2 —á–∞—Å–∞ –Ω–∞–∑–∞–¥", "1 –¥–µ–Ω—å –Ω–∞–∑–∞–¥")
            age_minutes = self._parse_age_to_minutes(age_text)
            
            return {
                'id': product_id,
                'title': title,
                'price_yuan': price_yuan,
                'price_rub': price_rub,
                'price_display': f"¬•{price_yuan:.2f} (~{price_rub:.0f} —Ä—É–±)",
                'url': url,
                'location': location,
                'age_minutes': age_minutes,
                'age_text': age_text,
                'search_query': search_query,
                'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M'),
                'images': [],  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
                'is_original': False,  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –ø–æ–∑–∂–µ
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
            return None
    
    def _parse_age_to_minutes(self, age_text: str) -> int:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤–æ–∑—Ä–∞—Å—Ç –≤ –º–∏–Ω—É—Ç—ã"""
        age_text = age_text.lower()
        
        if '–º–∏–Ω—É—Ç' in age_text or 'minute' in age_text:
            match = re.search(r'\d+', age_text)
            return int(match.group()) if match else 5
        elif '—á–∞—Å' in age_text or 'hour' in age_text:
            match = re.search(r'\d+', age_text)
            return int(match.group()) * 60 if match else 60
        elif '–¥–µ–Ω—å' in age_text or 'day' in age_text:
            match = re.search(r'\d+', age_text)
            return int(match.group()) * 1440 if match else 1440
        elif '–Ω–µ–¥–µ–ª' in age_text or 'week' in age_text:
            match = re.search(r'\d+', age_text)
            return int(match.group()) * 10080 if match else 10080
        else:
            return 60  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1 —á–∞—Å